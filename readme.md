# Проект _Анализ публикуемых новостей_

### Общая задача: создать ETL-процесс формирования витрин данных для анализа публикаций новостей.
#### Подробное описание задачи:
1. **Разработать скрипты загрузки данных в 2-х режимах**:
- Инициализирующий – загрузка полного слепка данных источника
- Инкрементальный – загрузка дельты данных за прошедшие сутки

2. **Организовать правильную структуру хранения данных**:
- Сырой слой данных
- Промежуточный слой
- Слой витрин

3. **В качестве результата работы программного продукта необходимо написать скрипт, который формирует витрину данных следующего содержания**:
- Суррогатный ключ категории
- Название категории
- Общее количество новостей из всех источников по данной категории за все время
- Количество новостей данной категории для каждого из источников за все время
- Общее количество новостей из всех источников по данной категории за последние сутки
- Количество новостей данной категории для каждого из источников за последние сутки
- Среднее количество публикаций по данной категории в сутки
- День, в который было сделано максимальное количество публикаций по данной категории
- Количество публикаций новостей данной категории по дням недели

Дополнение: *Т.к. в разных источниках названия и разнообразие категорий могут отличаться, вам необходимо привести все к единому виду.*

Источники:
https://lenta.ru/rss/, 
https://www.vedomosti.ru/rss/news,
https://tass.ru/rss/v2.xml

## Описание проекта

#### Используемые технологии
Так как проект предполагает выполнение по задач расписанию, необходимо использование оркестратора. В качестве основного был выбран вариант реализации проекта на базе docker контейнера airflow с дополнительным контейнером postgresql.
Для скрэпинга данных с интернет страниц и их обработки использовался python 3.8 с дополнительными библиотеками.

#### Структура проекта

В файле *docker-compose.yaml* содержатся настройки контейнера (_перед использованием проверьте номера портов_).

В файле *Dockerfile* - загрузка дополнительных библиотек python.

**Стандартные папки airflow:**
- ./dags - содержит файлы задач для airflow
- ./logs - содержит данные логов
- ./plugins - плагины airflow
**Дополнительные папки:**
- ./sql - содержит скрипты создания таблиц данных и их начального наполнения (опционально)
    - create_tables.sql - создание таблиц при инициации контейнера
    - fill_tables.sql - заполнение таблиц при инициации контейнера
- ./data - сырые данные в виде сжатых csv файлов 
    - файлы вида "news_НазваниеИсточника_Дата_Время_Зона.csv.gz" 
- ./postgres-data - данные postgres

**Папки python:**
- ./src/modules - находятся модули python, используемые (загружаемые) в DAGs в папке ./dags
    - _**LoadData_fin.py**_ - содержит функции данные для обработки данных интернет страниц
- ./src/test_data - результаты тестирования модулей (загрузки и обработки данных)

#### Зависимости
В файле requirements.txt перечислены стандартные модули python, используемые для загрузки и обработки данных:
- psycopg2
- psycopg2-binary
- bs4
- datetime
- pandas
- requests
- pytz
- regex
- sys
- sqlalchemy
- scikit-learn
- nltk

**Дополнение**: _В дальнейшем планируется использовать scikit-learn и nltk для автоматического назначения категорий для новостей по заголовкам._

#### Структура данных 

**Слои данных**
1. Слой с сырыми данными - папка ./data. В дальнейшем файлы со старыми датами могут быть перемещены в озеро данных или удалены.
2. Таблицы в postgresql:
    - sources - таблица новостных источников
    - old_categories - список старых значений категорий (как в источниках)
    - new_categories - список новых значений категорий новостей (после переименования)
    - category_changes - таблица связи старых и новых категорий для каждого источника
    - all_news - таблица с данными из новостных источников
    - 
3. Витрина данных в виде materialized view (обновляется раз в сутки)
    - view_news_summary 